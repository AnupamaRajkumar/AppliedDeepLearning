{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 1/B -- Keypoint detection on Cats Datset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnupamaRajkumar/AppliedDeepLearning/blob/master/Keypoint_detection_on_Cats_Datset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx1HWNSr5a-H"
      },
      "source": [
        "# Assignment 1/B\n",
        "**Disclaimer: Only for ADL/AML students!**\n",
        "\n",
        "### General information\n",
        "**You have to solve all tasks to pass!** \n",
        "\n",
        "Grade is calculated by the day of the last submission, but you will only get it after you've succesfully presented it. \n",
        "\n",
        "**Deadlines and grades:** \n",
        "  * 2020.09.20 - 2020.10.27 ==> 5\n",
        "  * 2020.10.28 - 2020.11.03 ==> 4\n",
        "  * 2020.10.04 - 2020.11.10 ==> 3\n",
        "  * 2020.11.11 - 2020.11.17 ==> 2\n",
        "  * 2020.11.18 or later ==> 1 \n",
        "\n",
        "You can **use only these** 3rd party **packages:** `cv2, keras, matplotlib, numpy, pandas, sklearn, skimage, tensorflow`.\n",
        "\n",
        "### Description\n",
        "In this assignment you have to build and train a cat eyes and nose keypoint detector model using tf.keras. We will use an autoencoder like architecture, which first encodes the data, then decodes it to its original size. To implement such kind of models, you should take a look at the following classes and methods: `Funcitonal API, MaxPooling2D, Conv2DTranspose`.\n",
        "\n",
        "### Use GPU\n",
        "Runtime -> Change runtime type\n",
        "\n",
        "At Hardware accelerator select  GPU then save it.  \n",
        "\n",
        "### Useful shortcuts\n",
        "* Run selected cell: *Ctrl + Enter*\n",
        "* Insert cell below: *Ctrl + M B*\n",
        "* Insert cell above: *Ctrl + M A*\n",
        "* Convert to text: *Ctrl + M M*\n",
        "* Split at cursor: *Ctrl + M -*\n",
        "* Autocomplete: *Ctrl + Space* or *Tab*\n",
        "* Move selected cells up: *Ctrl + M J*\n",
        "* Move selected cells down: *Ctrl + M K*\n",
        "* Delete selected cells: *Ctrl + M D*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwfba0if-93A"
      },
      "source": [
        "## Prepare dataset\n",
        "\n",
        "* Download the Cats dataset. We will only use a subset of the original dataset, the CAT_00 folder. Here you can find more information about the dataset: https://www.kaggle.com/crawford/cat-dataset\n",
        "* Preprocess the data. You can find some help here: https://github.com/kairess/cat_hipsterizer/blob/master/preprocess.py\n",
        "  * Following the steps in the link above, read and resize the images to be 128x128.\n",
        "  * Keep only the left eye, right eye and mouth coordinates.\n",
        "  * Create a keypoint heatmap from the keypoints. A 128x128x3 channel image, where the first channel corresponds to left eye heatmap, the sencond channel corresponds to the right eye heatmap and the third channel corresponds to the mouth heatmap. To do this:\n",
        "    1. At each keypoint, draw a circle with its corresponding color. For this, use the following method: `cv2.circle(<heatmap>, center=<keypoint_coord>, radius=2, color=<keypoint_color>, thickness=2)`\n",
        "    2. Then smooth the heatmap with a 5x5 Gauss filter: `<heatmap> = cv2.GaussianBlur(<heatmap>, (5,5), 0)` \n",
        "  * Then crop each image and heatmap:\n",
        "    1. Define the bounding box, select the min and max keypoint coordinates: `x1, y1, x2, y2`.\n",
        "    2. Add a 20x20 border around it: `x1, y1, x2, y2 = x1-20, y1-20, x2+20, y2+20`.\n",
        "    3. Then crop the image and the heatmap using the extended bounding box.  \n",
        "  * Finally, resize the images and the heatmaps to be 64x64.\n",
        "* Split the datasets into train-val-test sets (ratio: 60-20-20), without shuffling.\n",
        "* Print the size of each set and plot 5 training images and their corresponding masks.\n",
        "* Normalize the datasets. All value should be between 0.0 and 1.0. *Note: you don't have to use standardization, you can just divide them by 255.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsnF6rXiiQJl"
      },
      "source": [
        "# Download from Drive\n",
        "!if ! [ -f CAT_00.zip ]; then curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1wGwNi8t-UKAKs-LQL3dG-D8dzGVPHv2w\" > /dev/null; curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1wGwNi8t-UKAKs-LQL3dG-D8dzGVPHv2w\" -o CAT_00.zip; fi\n",
        "\n",
        "# Check if the file size is correct (~402MB)\n",
        "!if (( $(stat -c%s CAT_00.zip) < 421896648 )); then rm -rfd CAT_00.zip; fi\n",
        "\n",
        "# If not, download it from NIPG12\n",
        "!wget -nc -O CAT_00.zip http://nipg1.inf.elte.hu:8000/CAT_00.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQDTcEODmJDz"
      },
      "source": [
        "!unzip CAT_00.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwRGPNi0iQoQ"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas\n",
        "import skimage\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaB2OHsaYqj8"
      },
      "source": [
        "## Data augmentation\n",
        "  * Augment the training set using `ImageDataGenerator`. The parameters should be the following: `featurewise_center=False, featurewise_std_normalization=False, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.2`.\n",
        "  * When creating the generator(s), use shuffling with a seed value of 1 and batch size of 32.\n",
        "  * To validate that the augmentation is working, plot 5 original images with their corresponding transformed (augmented) images and masks.\n",
        "\n",
        "**Keep in mind:** To augment the inputs and targets the same way, you should create 2 separate generator, then you can zip them together. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui3_jaq2XcW5"
      },
      "source": [
        "## Define the model\n",
        "Define the following architecture in tf.keras:\n",
        "```\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
        "_________________________________________________________________\n",
        "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
        "_________________________________________________________________\n",
        "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
        "_________________________________________________________________\n",
        "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
        "_________________________________________________________________\n",
        "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
        "_________________________________________________________________\n",
        "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
        "_________________________________________________________________\n",
        "bottleneck_1 (Conv2D)        (None, 32, 32, 160)       5243040   \n",
        "_________________________________________________________________\n",
        "bottleneck_2 (Conv2D)        (None, 32, 32, 160)       25760     \n",
        "_________________________________________________________________\n",
        "upsample_1 (Conv2DTranspose) (None, 64, 64, 3)         1920      \n",
        "=================================================================\n",
        "Total params: 5,530,880\n",
        "Trainable params: 5,530,880\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```\n",
        "* Use relu and `padding='same'` for each layer.\n",
        "* Use a 2x2 `Conv2DTranspose` layer without bias to upsample the result. \n",
        "* `block1_conv1`, `block1_conv2`, `block2_conv1` and `block2_conv2` are 3x3 convolutions.\n",
        "* `bottleneck_1` is a 16x16 and `bottleneck_2` is a 1x1 convolution.\n",
        "* For optimizer use RMSProp, and MSE as loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQYj-7YuiT8w"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad9c6GsZatH9"
      },
      "source": [
        "def fcnet(input_size=(64, 64, 3)):\n",
        "  pass\n",
        "\n",
        "model = fcnet(input_size=(64, 64, 3))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkj5OLXrHGqK"
      },
      "source": [
        "## Training and evaluation \n",
        "  * Train the model for 30 epochs without early stopping.\n",
        "  * Plot the training curve (train/validation loss).\n",
        "  * Evaluate the trained model on the test set.\n",
        "  * Plot some (5) predictions with their corresponding GT heatmap and input. You should mark the location of each keypoint on the image. *Note: it might be worth to take a look at this answer: https://stackoverflow.com/a/17386204*"
      ]
    }
  ]
}