{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 1/D -- Anomaly detection on Simpsons and Flowers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnupamaRajkumar/AppliedDeepLearning/blob/master/Anomaly_detection_on_Simpsons_and_Flowers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx1HWNSr5a-H"
      },
      "source": [
        "# Assignment 1/D\n",
        "**Disclaimer: Only for ADL/AML students!**\n",
        "\n",
        "### General information\n",
        "**You have to solve all tasks to pass!** \n",
        "\n",
        "Grade is calculated by the day of the last submission, but you will only get it after you've succesfully presented it. \n",
        "\n",
        "**Deadlines and grades:** \n",
        "  * 2020.09.20 - 2020.10.27 ==> 5\n",
        "  * 2020.10.28 - 2020.11.03 ==> 4\n",
        "  * 2020.10.04 - 2020.11.10 ==> 3\n",
        "  * 2020.11.11 - 2020.11.17 ==> 2\n",
        "  * 2020.11.18 or later ==> 1 \n",
        "\n",
        "You can **use only these** 3rd party **packages:** `cv2, keras, matplotlib, numpy, sklearn, skimage, tensorflow`.\n",
        "\n",
        "### Description\n",
        "In this assignment you have to build a reconstruction loss based anomaly detection model using tf.keras. You have to train an autoencoder to reconstruct Homer Simpson images, then use the trained model to classify Simpson and Flower images. To implement such kind of models, you should take a look at the following classes and methods: `Sequential model, Funcitonal API, MaxPooling2D, Conv2DTranspose`.\n",
        "\n",
        "### Use GPU\n",
        "Runtime -> Change runtime type\n",
        "\n",
        "At Hardware accelerator select  GPU then save it.  \n",
        "\n",
        "### Useful shortcuts\n",
        "* Run selected cell: *Ctrl + Enter*\n",
        "* Insert cell below: *Ctrl + M B*\n",
        "* Insert cell above: *Ctrl + M A*\n",
        "* Convert to text: *Ctrl + M M*\n",
        "* Split at cursor: *Ctrl + M -*\n",
        "* Autocomplete: *Ctrl + Space* or *Tab*\n",
        "* Move selected cells up: *Ctrl + M J*\n",
        "* Move selected cells down: *Ctrl + M K*\n",
        "* Delete selected cells: *Ctrl + M D*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwfba0if-93A"
      },
      "source": [
        "## Prepare dataset\n",
        "\n",
        "### Simpsons\n",
        "* Download the Simpsons Characters dataset. Here you can find more information about the dataset: https://www.kaggle.com/alexattia/the-simpsons-characters-dataset\n",
        "* After extracting it, select only those images, on which Homer Simpson is present.\n",
        "* Resize all image to be 64x64.\n",
        "* Then split the datasets into train-val-test sets (ratio: 60-20-20), without shuffling.\n",
        "* Print the size of each set and plot 5 training images.\n",
        "* Normalize the datasets. All value should be between -1.0 and 1.0. *Note: you don't have to use standardization, you can just divide them by 255.*\n",
        "\n",
        "### Flowers\n",
        "* Download the Flowes Recognition dataset. Here you can find more information about the dataset: https://www.kaggle.com/alxmamaev/flowers-recognition\n",
        "* After extracting it, resize all image to be 64x64.\n",
        "* Print the size of each set and plot 5 training images.\n",
        "* And finally normalize the datasets. All value should be between -1.0 and 1.0. *Note: you don't have to use standardization, you can just divide them by 255.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsnF6rXiiQJl"
      },
      "source": [
        "# Simpsons characters\n",
        "\n",
        "# Download from Drive\n",
        "!if ! [ -f simpsons_dataset.zip ]; then curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1odJvgHZXoShkWN5s3FNE1nDW_7wzxS3J\" > /dev/null; curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1odJvgHZXoShkWN5s3FNE1nDW_7wzxS3J\" -o simpsons_dataset.zip; fi\n",
        "\n",
        "# Check if the file size is correct (~1.07GB)\n",
        "!if (( $(stat -c%s simpsons_dataset.zip) < 1158208931 )); then rm -rfd simpsons_dataset.zip; fi\n",
        "\n",
        "# If not, download it from NIPG12\n",
        "!wget -nc -O simpsons_dataset.zip http://nipg1.inf.elte.hu:8000/simpsons_dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQDTcEODmJDz"
      },
      "source": [
        "!unzip simpsons_dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLM53tf472N6"
      },
      "source": [
        "# Flowers Recgonition\n",
        "\n",
        "# Download from Drive\n",
        "!if ! [ -f flowers-recognition.zip ]; then curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1X1BDlF-Zuu7TIL9K4f7qOn7KGbS2_csw\" > /dev/null; curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1X1BDlF-Zuu7TIL9K4f7qOn7KGbS2_csw\" -o flowers-recognition.zip; fi\n",
        "\n",
        "# Check if the file size is correct (~224MB)\n",
        "!if (( $(stat -c%s flowers-recognition.zip) < 235781000 )); then rm -rfd flowers-recognition.zip; fi\n",
        "\n",
        "# If not, download it from NIPG12\n",
        "!wget -nc -O flowers-recognition.zip http://nipg12.inf.elte.hu:8000/flowers-recognition.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu3cDIP172fU"
      },
      "source": [
        "!unzip flowers-recognition.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwRGPNi0iQoQ"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas\n",
        "import skimage\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaB2OHsaYqj8"
      },
      "source": [
        "## Data augmentation\n",
        "  * Augment the training set using `ImageDataGenerator`. The parameters should be the following: `featurewise_center=False, featurewise_std_normalization=False, rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.2, horizontal_flip=True`.\n",
        "  * When creating the generator(s), use shuffling with a seed value of 0 and batch size of 128.\n",
        "  * To validate that the augmentation is working, plot 5 original images with their corresponding transformed (augmented) images.\n",
        "\n",
        "**Keep in mind:** Your task is to reconstuct images, so your target is your input. To augment the inputs and targets the same way, you should create 2 separate generator, then you can zip them together.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui3_jaq2XcW5"
      },
      "source": [
        "## Define the model\n",
        "Define the following architecture in tf.keras:\n",
        "```\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "conv2d (Conv2D)              (None, 64, 64, 32)        896       \n",
        "_________________________________________________________________\n",
        "conv2d_1 (Conv2D)            (None, 64, 64, 32)        9248      \n",
        "_________________________________________________________________\n",
        "max_pooling2d (MaxPooling2D) (None, 32, 32, 32)        0         \n",
        "_________________________________________________________________\n",
        "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
        "_________________________________________________________________\n",
        "conv2d_3 (Conv2D)            (None, 32, 32, 64)        36928     \n",
        "_________________________________________________________________\n",
        "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
        "_________________________________________________________________\n",
        "reshape (Reshape)            (None, 256, 64)           0         \n",
        "_________________________________________________________________\n",
        "dense (Dense)                (None, 256, 1024)         66560     \n",
        "_________________________________________________________________\n",
        "dense_1 (Dense)              (None, 256, 256)          262400    \n",
        "_________________________________________________________________\n",
        "reshape_1 (Reshape)          (None, 16, 16, 256)       0         \n",
        "_________________________________________________________________\n",
        "conv2d_transpose (Conv2DTran (None, 32, 32, 64)        147520    \n",
        "_________________________________________________________________\n",
        "conv2d_4 (Conv2D)            (None, 32, 32, 64)        36928     \n",
        "_________________________________________________________________\n",
        "conv2d_5 (Conv2D)            (None, 32, 32, 64)        36928     \n",
        "_________________________________________________________________\n",
        "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 32)        18464     \n",
        "_________________________________________________________________\n",
        "conv2d_6 (Conv2D)            (None, 64, 64, 32)        9248      \n",
        "_________________________________________________________________\n",
        "conv2d_7 (Conv2D)            (None, 64, 64, 32)        9248      \n",
        "_________________________________________________________________\n",
        "conv2d_8 (Conv2D)            (None, 64, 64, 3)         867       \n",
        "=================================================================\n",
        "Total params: 653,731\n",
        "Trainable params: 653,731\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```\n",
        "* Use relu, a kernel size of 3x3 and `padding='same'` for each layer.\n",
        "* Use a 3x3 `Conv2DTranspose` layer to upsample the result. \n",
        "* For optimizer use Adam, and MAE as loss function, and add MSE as a metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQYj-7YuiT8w"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad9c6GsZatH9"
      },
      "source": [
        "def ae_model(input_size=(64, 64, 3)):\n",
        "  pass\n",
        "\n",
        "model = ae_model(input_size=(64, 64, 3))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkj5OLXrHGqK"
      },
      "source": [
        "## Training and evaluation \n",
        "  * Train the model on the augmented data for 200 epochs without early stopping.\n",
        "  * Plot the training curve (train/validation loss and mse).\n",
        "  * Evaluate the trained model on the test set.\n",
        "  * Plot some (5) reconstruction examples. (Input and prediction pairs.)\n",
        "  * Calculate the mean and the std of the MAE loss on the **validation dataset** (*not on the loss history*), then set the classification threshold to the following: `<thrs> = <mean> + 0.5*<std>`. *Note: To get the mean and std, you have iterate over all validation samples and calculate the reconstruction loss on each.*\n",
        "  * Evaluate the classification performance on the test set.\n",
        "  * And finally evaluate the classification performance on the Flowers dataset. "
      ]
    }
  ]
}