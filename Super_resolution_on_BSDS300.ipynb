{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 1/E -- Super-resolution on BSDS300.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnupamaRajkumar/AppliedDeepLearning/blob/master/Super_resolution_on_BSDS300.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx1HWNSr5a-H"
      },
      "source": [
        "# Assignment 1/E\n",
        "**Disclaimer: Only for ADL/AML students!**\n",
        "\n",
        "### General information\n",
        "**You have to solve all tasks to pass!** \n",
        "\n",
        "Grade is calculated by the day of the last submission, but you will only get it after you've succesfully presented it. \n",
        "\n",
        "**Deadlines and grades:** \n",
        "  * 2020.09.20 - 2020.10.27 ==> 5\n",
        "  * 2020.10.28 - 2020.11.03 ==> 4\n",
        "  * 2020.10.04 - 2020.11.10 ==> 3\n",
        "  * 2020.11.11 - 2020.11.17 ==> 2\n",
        "  * 2020.11.18 or later ==> 1 \n",
        "\n",
        "You can **use only these** 3rd party **packages:** `cv2, pandas, keras, matplotlib, numpy, sklearn, skimage, tensorflow`.\n",
        "\n",
        "### Description\n",
        "In this assignment you have to build a model for super-resolution using tf.keras. You have to train a CNN to upscale small image patches, then use the trained model to upscale full images. To implement such kind of model, you should take a look at the following classes and methods: `Sequential model, Funcitonal API, Conv2D, cv2.cvtColor (BGR2YCrCb, YCrCb2RGB, YCrCb2BGR)`.\n",
        "\n",
        "### Use GPU\n",
        "Runtime -> Change runtime type\n",
        "\n",
        "At Hardware accelerator select  GPU then save it.  \n",
        "\n",
        "### Useful shortcuts\n",
        "* Run selected cell: *Ctrl + Enter*\n",
        "* Insert cell below: *Ctrl + M B*\n",
        "* Insert cell above: *Ctrl + M A*\n",
        "* Convert to text: *Ctrl + M M*\n",
        "* Split at cursor: *Ctrl + M -*\n",
        "* Autocomplete: *Ctrl + Space* or *Tab*\n",
        "* Move selected cells up: *Ctrl + M J*\n",
        "* Move selected cells down: *Ctrl + M K*\n",
        "* Delete selected cells: *Ctrl + M D*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwfba0if-93A"
      },
      "source": [
        "## Prepare dataset\n",
        "\n",
        "### BSDS300\n",
        "* LR: Low resolution, HR: high resolution, SR: super resolution\n",
        "* Download the BSDS300 dataset. Here you can find more information about the dataset: https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/\n",
        "* After extracting it, convert them to YCrCb color space. We will use the luminance information (0th channel) only as the input of the CNN.\n",
        "* Create 32x32 image patches with a sliding window (16 step size).\n",
        "* Use 0.15 part of the train set as the validation set.\n",
        "* The Y (0th) channel of the original images will be the ground truth. Create the LR images by resizing them half of the original size, then resize it back to the original size using cubic interpolation.\n",
        "* Print the size of each set and plot 5 LR training images and their corresponding HR images. *Note: Don't forget to convert the color space from YCrCb back to RGB before plotting.*\n",
        "* Normalize the datasets. All value should be between 0.0 and 1.0. *Note: you don't have to use standardization, you can just divide them by 255.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsnF6rXiiQJl"
      },
      "source": [
        "# BSDS300 dataset\n",
        "!wget https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/BSDS300-images.tgz\n",
        "# If not working, uncomment the following line:\n",
        "# !wget http://nipg12.inf.elte.hu:8000/BSDS300-images.tgz\n",
        "!tar -xvzf BSDS300-images.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwRGPNi0iQoQ"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas\n",
        "import skimage\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui3_jaq2XcW5"
      },
      "source": [
        "## Define the model\n",
        "Define the following architecture in tf.keras:\n",
        "```\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "conv2d (Conv2D)              (None, 32, 32, 128)       10496     \n",
        "_________________________________________________________________\n",
        "conv2d_1 (Conv2D)            (None, 32, 32, 64)        73792     \n",
        "_________________________________________________________________\n",
        "conv2d_2 (Conv2D)            (None, 32, 32, 1)         1601      \n",
        "=================================================================\n",
        "Total params: 85,889\n",
        "Trainable params: 85,889\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```\n",
        "* Use 9x9, 3x3 and 5x5 kernels with `relu`, `relu` and `linear` activations in `conv2d`, `conv2d_1` and `conv2d_2`, respectively.\n",
        "* For optimizer use Adam, and MSE as loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQYj-7YuiT8w"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad9c6GsZatH9"
      },
      "source": [
        "def train_model(input_size=(32, 32, 1)):\n",
        "  pass\n",
        "\n",
        "def predict_model(input_size=(None, None, 1)):\n",
        "  pass\n",
        "\n",
        "model = train_model(input_size=(32, 32, 1))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkj5OLXrHGqK"
      },
      "source": [
        "## Training and evaluation \n",
        "  * Create separate model definitions: one for training small image patches and the other is for predicting on full images.\n",
        "  * Train the model on the LR image patches for 10 epochs without early stopping.\n",
        "  * Plot the training curve (train/validation loss and mse).\n",
        "  * Evaluate the trained model on the test set.\n",
        "  * Create a prediction model, load the pretrained weights.\n",
        "  * Plot some (5) examples. (HR, LR [cubic], SR  [prediction] triplets.) \n",
        "  * Calculate commonly used super-resolution metrics between HR/LR and HR/SR images. The metrics are: Peak Signal-to-Noise Ratio (PSNR) [20*log10(255/rmse)], Structural Similarity (SSIM) [skimage.measure package, compare_ssim function] and the MSE as well.\n",
        "\n",
        "*Note: Don't forget to convert the color space from YCrCb back to RGB before plotting and metric calculation.*"
      ]
    }
  ]
}