{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2/C -- Emotion recognition on RAVDESS dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnupamaRajkumar/AppliedDeepLearning/blob/master/Emotion_recognition_on_RAVDESS_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx1HWNSr5a-H"
      },
      "source": [
        "\n",
        "# Assignment 2/C\n",
        "**Disclaimer: Only for ADL/AML students!**\n",
        "\n",
        "### General information\n",
        "**You have to solve all tasks to pass!**\n",
        "\n",
        "Grade is calculated by the day of the last submission, but you will only get it after you've succesfully presented it. \n",
        "\n",
        "*Deadlines and grades:* \n",
        "  * 2020.10.27 - 2020.11.24 ==> 5\n",
        "  * 2020.11.25 - 2020.12.01 ==> 4\n",
        "  * 2020.12.02 - 2020.12.08 ==> 3\n",
        "  * 2020.12.09 - 2020.12.15 ==> 2\n",
        "  * 2020.12.16 or later ==> 1\n",
        "\n",
        "You can **use only these** 3rd party **packages:** `cv2, keras, matplotlib, numpy, pandas, sklearn, skimage, tensorflow, librosa`.\n",
        "\n",
        "### Description\n",
        "In this assignment you have to build a multimodal deep neural network for emotion detection using tf.keras. You have to work with the RAVDESS dataset, which contains short (~4 seconds long) video clip recordings of speakers, who are acting the different emotions through 2 sentences. We will extract and combine RGB frames with MFCCs and utilize both video and audio information sources to achieve a better prediction.\n",
        "\n",
        "### Use GPU\n",
        "Runtime -> Change runtime type\n",
        "\n",
        "At Hardware accelerator select  GPU then save it.  \n",
        "\n",
        "### Useful shortcuts\n",
        "* Run selected cell: *Ctrl + Enter*\n",
        "* Insert cell below: *Ctrl + M B*\n",
        "* Insert cell above: *Ctrl + M A*\n",
        "* Convert to text: *Ctrl + M M*\n",
        "* Split at cursor: *Ctrl + M -*\n",
        "* Autocomplete: *Ctrl + Space* or *Tab*\n",
        "* Move selected cells up: *Ctrl + M J*\n",
        "* Move selected cells down: *Ctrl + M K*\n",
        "* Delete selected cells: *Ctrl + M D*\n",
        "\n",
        "If you have any question, feel free to ask.\n",
        "fodorad201@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwfba0if-93A"
      },
      "source": [
        "## Prepare dataset\n",
        "\n",
        "* Download the RAVDESS dataset. Here you can find more information about the dataset: https://zenodo.org/record/1188976#.X5g53OLPw2w\n",
        "The dataset is available here as well: http://nipg1.inf.elte.hu:8765\n",
        "ravdess.zip contains all of the mp4 clips. The labels are in the file names. (classification task)\n",
        "\n",
        "* Preprocess the data.\n",
        "  * Remove the silence parts from the beginning and the end of video clips. (Tips: ffmpeg filters)\n",
        "  * Audio representation: \n",
        "    * Extract the audio from the video. (Tips: ffmpeg)\n",
        "    * Extract 24 Mel Frequency Cepstral Coefficients from the audio. (Tips: use librosa.)\n",
        "    * Calculate the mean number of (spectral) frames in the dataset.\n",
        "    * Standardize the MFCCs sample-wise. (Tips: zero mean and unit variance)\n",
        "    * Use pre-padding (Note: with 0, which is also the mean after standardization) to unify the length of the samples.\n",
        "    * Audio representation per sample is a tensor with shape (N,M,1) where N is the number of coefficients (e.g. 24) and M is the number of audio frames.\n",
        "  * Visual representation:\n",
        "    * Extract the faces from the images. (Tips: You can use the cv2.CascadeClassifier, or the DLIB package to determine facial keypoints, or MTCNN to predict bounding boxes.)\n",
        "    * Resize the face images to 64x64. (Tips: You can use lower/higher resolution as well.)\n",
        "    * Subsample the frames to reduce complexity (6 frames/video is enough).\n",
        "    * Apply data augmentation, and scaling [0, 1].\n",
        "    * Video representation per sample is a tensor with shape (F,H,W,3) where F is the number of frames (e.g. 6), H and W are the spatial dimensions (e.g. 64).\n",
        "  * Ground truth labels:\n",
        "    * There are 8 class labels. However, Class 1 (Neutral) and Class 2 (Calm) are almost the same. It is a commonly used practice to merge these two classes. Combine them to reduce complexity.\n",
        "    * (Optional) Use one-hot-encoding with categorical_crossentropy loss later on, or keep them between [0, 6] and use sparse_categorical_crossentropy loss. It's up to you.\n",
        "\n",
        "* Split the datasets into train-valid-test sets. Samples from the same speaker shouldn't appear in multiple sets. (Example split using speaker ids: 1-17: train set, 18-22: validation set, 23-24: test set)\n",
        "* Create a generator, which iterates over the audio and visual representations. (Note: the generator should produce a tuple ([x0, x1], y), where x0 is the audio, x1 is the video representation, y is the ground truth.\n",
        "* Print the size of each set, plot 3 samples: frames, MFCCs and their corresponding emotion class labels. (Tips: use librosa for plotting MFCCs)\n",
        "\n",
        "Alternative considerations. They may require additional steps:\n",
        "* You can use Mean (axis=1) MFCCs vectors to further reduce complexity. Input of the corresponding subnetwork should be modified to accept inputs with shape (N, 1).\n",
        "* You can use log-melspectrograms as well. Note, that raw spectrograms are displaying power. Mel scale should be applied on the frequency axis, and log on the third dimension (decibels are expected). You can use librosa for that (librosa.feature.melspectrogram, librosa.power_to_db)\n",
        "* A better evaluation procedure here is the LOO (Leave-One-Out) cross-validation, however it can be costy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuONHKhuhpD2"
      },
      "source": [
        "## Create Model\n",
        "\n",
        "* Create the audio subnetwork\n",
        "  * Choose one of these:\n",
        "    * BLSTM (64 units, return sequences) + Dropout 0.5 + BLSTM (64 units) + Dense (128 units, ReLU)\n",
        "    * Conv1D (32 filters, 3x3) + BatchNorm + ReLU, Conv1D (32 filters, 3x3) + BatchNorm + ReLU, Conv1D (64 filters, 3x3) + BatchNorm + ReLU, LSTM (64 units) + Dropout 0.5 + Dense (128 units, ReLU)\n",
        "    * Conv2D (32 filters, 3x3) + BatchNorm + ReLU, MaxPool2D, Conv2D (32 filters, 3x3) + BatchNorm + ReLU, MaxPool2D, Flatten, Dense (128 units, ReLU)\n",
        "  * You can try other configurations, better submodels. Have a reason for your choice!\n",
        "* Create the visual subnetwork\n",
        "  * Choose a visual backbone, which is applied frame-wise (Tips: use TimeDistributed Layer for this):\n",
        "    * VGG-like architecture (Conv2D + MaxPooling blocks)\n",
        "    * ResNet / Inception architecture (Residual blocks, Inception cells)\n",
        "  * You can try other configurations, better submodels (like 3D convolution nets). Have a reason for your choice!\n",
        "  * Apply Max pooling over the time dimension to reduce complexity (or use GRU or LSTM for better temporal modelling)\n",
        "* Model fusion:\n",
        "  * Concatenate the final hidden representations of the audio and visual subnetwork.\n",
        "  * Apply fully connected layers on it (256 units, ReLU), then an another dense layer (7 units, softmax).\n",
        "  * You can feed multiple inputs to the Model using a list: \n",
        "  model = tf.keras.models.Model(inputs=[input_audio, input_video], outputs=output)\n",
        "\n",
        "## Extra task (Optional)\n",
        "Use the VGGFace2 model (and pretrained weights) in the visual subnetwork. It is trained on faces, so a much better representation can be obtained with it. Finetune the network for enhanced prediction.\n",
        "(code: https://github.com/rcmalli/keras-vggface, but other implementation can be used as well)\n",
        "Note, that this repository use the classic keras, while we are using tf.keras.\n",
        "It may rise compatibility problems.\n",
        "\n",
        "**If you can successfully use the VGGFace2 pretrained net, +1 is added to the final grade.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNygXxrNpPJY"
      },
      "source": [
        "## Additional notes\n",
        "\n",
        "* Do the preprocessing steps offline, create pkl (or npy, hdf5, etc..) files, so you don't have to repeat most of the steps again. Then you can upload it, and train using colab without much struggling.\n",
        "* Use Adam optimizer.\n",
        "* Use learning rate scheduler.\n",
        "* Check the training curve. If overfitting happens, add more regularization: weight decay (L2: 1e-3, 5e-4, etc...), Dropout\n",
        "\n",
        "## Final steps, evaluation\n",
        "\n",
        "* Plot the training / validation curve.\n",
        "* Calculate accuracy, print a confusion matrix."
      ]
    }
  ]
}